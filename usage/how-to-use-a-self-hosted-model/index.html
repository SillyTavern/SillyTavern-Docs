<!DOCTYPE html>
<html lang="en" class="h-full">
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-0NTJMWH0TW1"></script>
    <script data-cfasync="false">window.dataLayer = window.dataLayer || [];function gtag(){dataLayer.push(arguments);}gtag('js', new Date());gtag('config', 'G-0NTJMWH0TW1');</script>

    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <meta name="turbo-cache-control" content="no-cache" data-turbo-track="reload" data-track-token="3.8.1.801516199812">

    <!-- See retype.com -->
    <meta name="generator" content="Retype 3.8.1">

    <!-- Primary Meta Tags -->
    <title>Self-hosted AI models | docs.ST.app</title>
    <meta name="title" content="Self-hosted AI models | docs.ST.app">
    <meta name="description" content="This guide is based on the author's personal experience and knowledge and is not an absolute truth.">

    <!-- Canonical -->
    <link rel="canonical" href="https://docs.sillytavern.app/usage/how-to-use-a-self-hosted-model/">

    <!-- Open Graph / Facebook -->
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://docs.sillytavern.app/usage/how-to-use-a-self-hosted-model/">
    <meta property="og:title" content="Self-hosted AI models | docs.ST.app">
    <meta property="og:description" content="This guide is based on the author's personal experience and knowledge and is not an absolute truth.">

    <!-- Twitter -->
    <meta property="twitter:card" content="summary_large_image">
    <meta property="twitter:url" content="https://docs.sillytavern.app/usage/how-to-use-a-self-hosted-model/">
    <meta property="twitter:title" content="Self-hosted AI models | docs.ST.app">
    <meta property="twitter:description" content="This guide is based on the author's personal experience and knowledge and is not an absolute truth.">

    <script data-cfasync="false">(function () { var el = document.documentElement, m = localStorage.getItem("doc_theme"), wm = window.matchMedia; if (m === "dark" || (!m && wm && wm("(prefers-color-scheme: dark)").matches)) { el.classList.add("dark") } else { el.classList.remove("dark") } })();</script>

    <link href="../../resources/css/retype.css?v=3.8.1.801516199812" rel="stylesheet">

    <script data-cfasync="false" src="../../resources/js/config.js?v=3.8.1.801516199812" data-turbo-eval="false" defer></script>
    <script data-cfasync="false" src="../../resources/js/retype.js?v=3.8.1" data-turbo-eval="false" defer></script>
    <script id="lunr-js" data-cfasync="false" src="../../resources/js/lunr.js?v=3.8.1.801516199812" data-turbo-eval="false" defer></script>

    <style>
        a {
            color: brown !important;
        }

        .dark a {
            color: rgb(223, 151, 18) !important;
        }

        body {
            font-family: Noto Sans, sans-serif;
        }

        .callout {
            padding: 1rem;
            margin-bottom: 1rem;
            border: 1px solid #d2d6dc;
            border-radius: 0.375rem;
        }

        .callout p {
            margin-bottom: 0.5rem !important;
            overflow: hidden;
        }

        .callout p:last-child {
            margin-bottom: 0 !important;
        }

        .callout .fa-fw {
            margin: 1rem 0.5rem 1rem 0;
            color: slategrey;
        }

        .docs-markdown .codeblock code,
        .docs-markdown .codeblock pre {
            white-space: pre-wrap;
        }
    </style>
    <link href="/static/webfonts/NotoSans/stylesheet.css" rel="stylesheet" />
    <link href="/static/css/fontawesome.min.css" rel="stylesheet" />
    <link href="/static/css/solid.min.css" rel="stylesheet" />
    <link href="/static/css/brands.min.css" rel="stylesheet" />

</head>
<body>
    <div id="docs-app" class="relative text-base antialiased text-gray-700 bg-white font-body dark:bg-dark-850 dark:text-dark-300">
        <div class="absolute bottom-0 left-0 bg-gray-100 dark:bg-dark-800" style="top: 5rem; right: 50%"></div>
    
        <header id="docs-site-header" class="sticky top-0 z-30 flex w-full h-16 bg-white border-b border-gray-200 md:h-20 dark:bg-dark-850 dark:border-dark-650">
            <div class="container relative flex items-center justify-between pr-6 grow md:justify-start">
                <!-- Mobile menu button skeleton -->
                <button v-cloak class="skeleton docs-mobile-menu-button flex items-center justify-center shrink-0 overflow-hidden dark:text-white focus:outline-none rounded-full w-10 h-10 ml-3.5 md:hidden"><svg xmlns="http://www.w3.org/2000/svg" class="mb-px shrink-0" width="24" height="24" viewBox="0 0 24 24" role="presentation" style="margin-bottom: 0px;"><g fill="currentColor"><path d="M2 4h20v2H2zM2 11h20v2H2zM2 18h20v2H2z"></path></g></svg></button>
                <div v-cloak id="docs-sidebar-toggle"></div>
        
                <!-- Logo -->
                <div class="flex items-center justify-between h-full py-2 md:w-75">
                    <div class="flex items-center px-2 md:px-6">
                        <a id="docs-site-logo" href="../../" class="flex items-center leading-snug text-xl">
                            <span class="w-10 mr-2 grow-0 shrink-0 overflow-hidden">
                                <img class="max-h-10 dark:hidden md:inline-block" src="../../static/logo.png">
                                <img class="max-h-10 hidden dark:inline-block" src="../../static/logo.png">
                            </span>
                            <span class="dark:text-white font-bold line-clamp-1 md:line-clamp-2">SillyTavern Documentation</span>
                        </a>
                    </div>
        
                    <span class="hidden h-8 border-r md:inline-block dark:border-dark-650"></span>
                </div>
        
                <div class="flex justify-between md:grow">
                    <!-- Top Nav -->
                    <nav class="hidden md:flex">
                        <ul class="flex flex-col mb-4 md:pl-16 md:mb-0 md:flex-row md:items-center">
                            <li class="mr-6">
                                <a class="py-2 md:mb-0 inline-flex items-center text-sm whitespace-nowrap transition-colors duration-200 ease-linear md:text-blue-500 dark:text-blue-400 hover:text-blue-800 dark:hover:text-blue-200" href="https://github.com/SillyTavern/SillyTavern">GitHub</a>
                            </li>
                            <li class="mr-6">
                                <a class="py-2 md:mb-0 inline-flex items-center text-sm whitespace-nowrap transition-colors duration-200 ease-linear md:text-blue-500 dark:text-blue-400 hover:text-blue-800 dark:hover:text-blue-200" href="https://discord.gg/sillytavern">Discord</a>
                            </li>
                            <li class="mr-6">
                                <a class="py-2 md:mb-0 inline-flex items-center text-sm whitespace-nowrap transition-colors duration-200 ease-linear md:text-blue-500 dark:text-blue-400 hover:text-blue-800 dark:hover:text-blue-200" href="https://www.reddit.com/r/SillyTavernAI/">Reddit</a>
                            </li>
        
                        </ul>
                    </nav>
        
                    <!-- Header Right Skeleton -->
                    <div v-cloak class="flex justify-end grow skeleton">
        
                        <!-- Search input mock -->
                        <div class="relative hidden w-40 lg:block lg:max-w-sm lg:ml-auto">
                            <div class="absolute flex items-center justify-center h-full pl-3 dark:text-dark-300">
                                <svg xmlns="http://www.w3.org/2000/svg" class="icon-base" width="16" height="16" viewBox="0 0 24 24" aria-labelledby="icon" role="presentation" style="margin-bottom: 1px;"><g fill="currentColor" ><path d="M21.71 20.29l-3.68-3.68A8.963 8.963 0 0020 11c0-4.96-4.04-9-9-9s-9 4.04-9 9 4.04 9 9 9c2.12 0 4.07-.74 5.61-1.97l3.68 3.68c.2.19.45.29.71.29s.51-.1.71-.29c.39-.39.39-1.03 0-1.42zM4 11c0-3.86 3.14-7 7-7s7 3.14 7 7c0 1.92-.78 3.66-2.04 4.93-.01.01-.02.01-.02.01-.01.01-.01.01-.01.02A6.98 6.98 0 0111 18c-3.86 0-7-3.14-7-7z" ></path></g></svg>
                            </div>
                            <input class="w-full h-10 placeholder-gray-400 transition-colors duration-200 ease-in bg-gray-200 border border-transparent rounded md:text-sm hover:bg-white hover:border-gray-300 focus:outline-none focus:bg-white focus:border-gray-500 dark:bg-dark-600 dark:border-dark-600 dark:placeholder-dark-400" style="padding: 0.625rem 0.75rem 0.625rem 2rem" type="text" placeholder="Search">
                        </div>
        
                        <!-- Mobile search button -->
                        <div class="flex items-center justify-center w-10 h-10 lg:hidden">
                            <svg xmlns="http://www.w3.org/2000/svg" class="shrink-0 icon-base" width="20" height="20" viewBox="0 0 24 24" aria-labelledby="icon" role="presentation" style="margin-bottom: 0px;"><g fill="currentColor" ><path d="M21.71 20.29l-3.68-3.68A8.963 8.963 0 0020 11c0-4.96-4.04-9-9-9s-9 4.04-9 9 4.04 9 9 9c2.12 0 4.07-.74 5.61-1.97l3.68 3.68c.2.19.45.29.71.29s.51-.1.71-.29c.39-.39.39-1.03 0-1.42zM4 11c0-3.86 3.14-7 7-7s7 3.14 7 7c0 1.92-.78 3.66-2.04 4.93-.01.01-.02.01-.02.01-.01.01-.01.01-.01.02A6.98 6.98 0 0111 18c-3.86 0-7-3.14-7-7z" ></path></g></svg>
                        </div>
        
                        <!-- Dark mode switch placeholder -->
                        <div class="w-10 h-10 lg:ml-2"></div>
        
                        <!-- History button -->
                        <div class="flex items-center justify-center w-10 h-10" style="margin-right: -0.625rem;">
                            <svg xmlns="http://www.w3.org/2000/svg" class="shrink-0 icon-base" width="22" height="22" viewBox="0 0 24 24" aria-labelledby="icon" role="presentation" style="margin-bottom: 0px;"><g fill="currentColor" ><g ><path d="M12.01 6.01c-.55 0-1 .45-1 1V12a1 1 0 00.4.8l3 2.22a.985.985 0 001.39-.2.996.996 0 00-.21-1.4l-2.6-1.92V7.01c.02-.55-.43-1-.98-1z"></path><path d="M12.01 1.91c-5.33 0-9.69 4.16-10.05 9.4l-.29-.26a.997.997 0 10-1.34 1.48l1.97 1.79c.19.17.43.26.67.26s.48-.09.67-.26l1.97-1.79a.997.997 0 10-1.34-1.48l-.31.28c.34-4.14 3.82-7.41 8.05-7.41 4.46 0 8.08 3.63 8.08 8.09s-3.63 8.08-8.08 8.08c-2.18 0-4.22-.85-5.75-2.4a.996.996 0 10-1.42 1.4 10.02 10.02 0 007.17 2.99c5.56 0 10.08-4.52 10.08-10.08.01-5.56-4.52-10.09-10.08-10.09z"></path></g></g></svg>
                        </div>
                    </div>
        
                    <div v-cloak class="flex justify-end grow">
                        <div id="docs-mobile-search-button"></div>
                        <doc-search-desktop></doc-search-desktop>
        
                        <doc-theme-switch class="lg:ml-2"></doc-theme-switch>
                        <doc-history></doc-history>
                    </div>
                </div>
            </div>
        </header>
    
        <div class="container relative flex bg-white">
            <!-- Sidebar Skeleton -->
            <div v-cloak class="fixed flex flex-col shrink-0 duration-300 ease-in-out bg-white border-gray-200 sidebar top-20 w-75 border-r h-screen md:sticky transition-transform skeleton dark:bg-dark-800 dark:border-dark-650">
            
                <div class="flex items-center h-16 px-6">
                    <input class="w-full h-8 px-3 py-2 transition-colors duration-200 ease-linear bg-white border border-gray-200 rounded shadow-none text-sm focus:outline-none focus:border-gray-600 dark:bg-dark-600 dark:border-dark-600" type="text" placeholder="Filter">
                </div>
            
                <div class="pl-6 mt-1 mb-4">
                    <div class="w-32 h-3 mb-4 bg-gray-200 rounded-full loading dark:bg-dark-600"></div>
                    <div class="w-48 h-3 mb-4 bg-gray-200 rounded-full loading dark:bg-dark-600"></div>
                    <div class="w-40 h-3 mb-4 bg-gray-200 rounded-full loading dark:bg-dark-600"></div>
                    <div class="w-32 h-3 mb-4 bg-gray-200 rounded-full loading dark:bg-dark-600"></div>
                    <div class="w-48 h-3 mb-4 bg-gray-200 rounded-full loading dark:bg-dark-600"></div>
                    <div class="w-40 h-3 mb-4 bg-gray-200 rounded-full loading dark:bg-dark-600"></div>
                </div>
            
                <div class="shrink-0 mt-auto bg-transparent dark:border-dark-650">
                    <a class="flex items-center justify-center flex-nowrap h-16 text-gray-400 dark:text-dark-400 hover:text-gray-700 dark:hover:text-dark-300 transition-colors duration-150 ease-in docs-powered-by" target="_blank" href="https://retype.com/" rel="noopener">
                        <span class="text-xs whitespace-nowrap">Powered by</span>
                        <svg xmlns="http://www.w3.org/2000/svg" class="ml-2" fill="currentColor" width="96" height="20" overflow="visible"><path d="M0 0v20h13.59V0H0zm11.15 17.54H2.44V2.46h8.71v15.08zM15.8 20h2.44V4.67L15.8 2.22zM20.45 6.89V20h2.44V9.34z"/><g><path d="M40.16 8.44c0 1.49-.59 2.45-1.75 2.88l2.34 3.32h-2.53l-2.04-2.96h-1.43v2.96h-2.06V5.36h3.5c1.43 0 2.46.24 3.07.73s.9 1.27.9 2.35zm-2.48 1.1c.26-.23.38-.59.38-1.09 0-.5-.13-.84-.4-1.03s-.73-.28-1.39-.28h-1.54v2.75h1.5c.72 0 1.2-.12 1.45-.35zM51.56 5.36V7.2h-4.59v1.91h4.13v1.76h-4.13v1.92h4.74v1.83h-6.79V5.36h6.64zM60.09 7.15v7.48h-2.06V7.15h-2.61V5.36h7.28v1.79h-2.61zM70.81 14.64h-2.06v-3.66l-3.19-5.61h2.23l1.99 3.45 1.99-3.45H74l-3.19 5.61v3.66zM83.99 6.19c.65.55.97 1.4.97 2.55s-.33 1.98-1 2.51-1.68.8-3.04.8h-1.23v2.59h-2.06V5.36h3.26c1.42 0 2.45.28 3.1.83zm-1.51 3.65c.25-.28.37-.69.37-1.22s-.16-.92-.48-1.14c-.32-.23-.82-.34-1.5-.34H79.7v3.12h1.38c.68 0 1.15-.14 1.4-.42zM95.85 5.36V7.2h-4.59v1.91h4.13v1.76h-4.13v1.92H96v1.83h-6.79V5.36h6.64z"/></g></svg>
                    </a>
                </div>
            </div>
            
            <!-- Sidebar component -->
            <doc-sidebar v-cloak>
                <template #sidebar-footer>
                    <div class="shrink-0 mt-auto border-t md:bg-transparent md:border-none dark:border-dark-650">
            
                        <div class="py-3 px-6 md:hidden border-b dark:border-dark-650">
                            <nav>
                                <ul class="flex flex-wrap justify-center items-center">
                                    <li class="mr-6">
                                        <a class="block py-1 inline-flex items-center text-sm whitespace-nowrap transition-colors duration-200 ease-linear md:text-blue-500 dark:text-blue-400 hover:text-blue-800 dark:hover:text-blue-200" href="https://github.com/SillyTavern/SillyTavern">GitHub</a>
                                    </li>
                                    <li class="mr-6">
                                        <a class="block py-1 inline-flex items-center text-sm whitespace-nowrap transition-colors duration-200 ease-linear md:text-blue-500 dark:text-blue-400 hover:text-blue-800 dark:hover:text-blue-200" href="https://discord.gg/sillytavern">Discord</a>
                                    </li>
                                    <li class="mr-6">
                                        <a class="block py-1 inline-flex items-center text-sm whitespace-nowrap transition-colors duration-200 ease-linear md:text-blue-500 dark:text-blue-400 hover:text-blue-800 dark:hover:text-blue-200" href="https://www.reddit.com/r/SillyTavernAI/">Reddit</a>
                                    </li>
            
                                </ul>
                            </nav>
                        </div>
            
                        <a class="flex items-center justify-center flex-nowrap h-16 text-gray-400 dark:text-dark-400 hover:text-gray-700 dark:hover:text-dark-300 transition-colors duration-150 ease-in docs-powered-by" target="_blank" href="https://retype.com/" rel="noopener">
                            <span class="text-xs whitespace-nowrap">Powered by</span>
                            <svg xmlns="http://www.w3.org/2000/svg" class="ml-2" fill="currentColor" width="96" height="20" overflow="visible"><path d="M0 0v20h13.59V0H0zm11.15 17.54H2.44V2.46h8.71v15.08zM15.8 20h2.44V4.67L15.8 2.22zM20.45 6.89V20h2.44V9.34z"/><g><path d="M40.16 8.44c0 1.49-.59 2.45-1.75 2.88l2.34 3.32h-2.53l-2.04-2.96h-1.43v2.96h-2.06V5.36h3.5c1.43 0 2.46.24 3.07.73s.9 1.27.9 2.35zm-2.48 1.1c.26-.23.38-.59.38-1.09 0-.5-.13-.84-.4-1.03s-.73-.28-1.39-.28h-1.54v2.75h1.5c.72 0 1.2-.12 1.45-.35zM51.56 5.36V7.2h-4.59v1.91h4.13v1.76h-4.13v1.92h4.74v1.83h-6.79V5.36h6.64zM60.09 7.15v7.48h-2.06V7.15h-2.61V5.36h7.28v1.79h-2.61zM70.81 14.64h-2.06v-3.66l-3.19-5.61h2.23l1.99 3.45 1.99-3.45H74l-3.19 5.61v3.66zM83.99 6.19c.65.55.97 1.4.97 2.55s-.33 1.98-1 2.51-1.68.8-3.04.8h-1.23v2.59h-2.06V5.36h3.26c1.42 0 2.45.28 3.1.83zm-1.51 3.65c.25-.28.37-.69.37-1.22s-.16-.92-.48-1.14c-.32-.23-.82-.34-1.5-.34H79.7v3.12h1.38c.68 0 1.15-.14 1.4-.42zM95.85 5.36V7.2h-4.59v1.91h4.13v1.76h-4.13v1.92H96v1.83h-6.79V5.36h6.64z"/></g></svg>
                        </a>
                    </div>
                </template>
            </doc-sidebar>
    
            <div class="grow min-w-0 dark:bg-dark-850">
                <!-- Render "toolbar" template here on api pages --><!-- Render page content -->
                <div class="flex">
                    <div class="min-w-0 p-4 grow md:px-16">
                        <main class="relative pb-12 lg:pt-2">
                            <div class="docs-markdown" id="docs-content">
                                <!-- Rendered if sidebar right is enabled -->
                                <div id="docs-sidebar-right-toggle"></div>
                                <!-- Page content  -->
<doc-anchor-target id="self-hosted-ai-models" class="break-words">
    <h1>
        <doc-anchor-trigger class="header-anchor-trigger" to="#self-hosted-ai-models">#</doc-anchor-trigger>
        <span>Self-hosted AI models</span>
    </h1>
</doc-anchor-target>
<div class="flex mb-6">
    <div class="shrink-0 w-1 rounded-tl rounded-bl bg-yellow-500"></div>
    <div class="flex w-full py-4 border border-l-0 border-gray-300 rounded-tr rounded-br doc-alert bg-white dark:bg-dark-700 dark:border-dark-700" role="alert">
        <div class="flex items-center ml-4 h-7">
            <svg xmlns="http://www.w3.org/2000/svg" class="mb-px text-yellow-500" width="22" height="22" viewBox="0 0 24 24" role="presentation">
                <g fill="currentColor"><g>
                    <path d="M22.48 15.59L14.01 1.45A2.968 2.968 0 0012.16.09c-.78-.19-1.58-.07-2.27.35-.41.25-.76.6-1.01 1.01v.01L.4 15.6c-.83 1.43-.33 3.27 1.1 4.1.45.26.95.4 1.48.4h16.95c.8-.01 1.55-.33 2.11-.9.56-.57.87-1.33.86-2.13a3.04 3.04 0 00-.42-1.48zm-1.87 2.21c-.19.19-.44.3-.69.3H2.99c-.17 0-.34-.05-.49-.13a.992.992 0 01-.37-1.35L10.6 2.48c.08-.14.2-.25.34-.33a.992.992 0 011.37.33l8.46 14.13c.09.15.13.32.13.49 0 .26-.1.51-.29.7z"></path>
                    <path d="M11.45 12.1c.55 0 1-.45 1-1v-4c0-.55-.45-1-1-1s-1 .45-1 1v4c0 .56.45 1 1 1zM11.46 14.1c-.56 0-1 .45-1 1s.45 1 1 1 1-.45 1-1-.45-1-1-1z"></path>
                </g></g>
            </svg>
        </div>
        <div class="pr-5 ml-3 w-full">
<p>This guide is based on the author&#x27;s personal experience and knowledge and is not an absolute truth. All statements should be taken with a grain of salt. If you have any corrections or suggestions, please contact us on Discord or send a PR to the <a href="https://github.com/SillyTavern/SillyTavern-Docs">SillyTavern documentation repository</a>.</p>
        </div>
    </div>
</div>
<doc-anchor-target id="intro">
    <h2>
        <doc-anchor-trigger class="header-anchor-trigger" to="#intro">#</doc-anchor-trigger>
        <span>Intro</span>
    </h2>
</doc-anchor-target>
<p>This guide aims to help you get set up using SillyTavern with a local AI running on your PC (we&#x27;ll start using the proper terminology from now on and call it an LLM). Read it before bothering people with tech support questions.</p>
<doc-anchor-target id="what-are-the-best-large-language-models">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#what-are-the-best-large-language-models">#</doc-anchor-trigger>
        <span>What are the best Large Language Models?</span>
    </h3>
</doc-anchor-target>
<p>It is impossible to answer this question as there&#x27;s no standardized scale of &quot;Best&quot;. The community has enough resources and discussions going on Reddit and Discord to form at least some opinion on what is the preferred / go-to model. Your mileage may vary.</p>
<doc-anchor-target id="what-is-the-best-configuration">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#what-is-the-best-configuration">#</doc-anchor-trigger>
        <span>What is the best configuration?</span>
    </h3>
</doc-anchor-target>
<p>If there was a best or no-brainer setup, would there even have to be a need for configuration? The best configuration is the one that works for you. It&#x27;s a trial-and-error process.</p>
<doc-anchor-target id="hardware-requirements-and-orientation">
    <h2>
        <doc-anchor-trigger class="header-anchor-trigger" to="#hardware-requirements-and-orientation">#</doc-anchor-trigger>
        <span>Hardware requirements and orientation</span>
    </h2>
</doc-anchor-target>
<p>This is a complex subject, so I&#x27;ll stick to the essentials and generalize.</p>
<ul>
<li>There are thousands of free LLMs you can download from the Internet, similar to how Stable Diffusion has tons of models you can get to generate images.</li>
<li>Running an unmodified LLM requires a monster GPU with a ton of VRAM (GPU memory). More than you will ever have.</li>
<li>It is possible to reduce VRAM requirements by compressing the model using quantization techniques, such as GPTQ or AWQ. This makes the model somewhat less capable, but greatly reduces the VRAM requirements to run it. Suddenly, this allowed people with gaming GPUs like a 3080 to run a 13B model. Even though it&#x27;s not as good as the unquantized model, it&#x27;s still good.</li>
<li>It gets better: there also exists a model format and quantization called GGUF (previously GGML) which has become the format of choice for normal people without monster GPUs. This allows you to use an LLM without a GPU at all. It will only use CPU and RAM. This is much slower (probably 15 times) than running the LLM on a GPU using GPTQ/AWQ, especially during the prompt processing, but the model&#x27;s ability is just as good. The GGUF creator then optimized GGUF further by adding a configuration option that allows people with a gaming-grade GPU to offload parts of the model to the GPU, allowing them to run part of the model at GPU speed (note that this doesn&#x27;t reduce RAM requirements, it only improves your generation speed).</li>
<li>There are different sizes of models, named based on the number of parameters they were trained with. You will see names like 7B, 13B, 30B, 70B, etc. You can think of these as the brain size of the model. A 13B model will be more capable than the 7B from the same family of models: they were trained on the same data, but the bigger brain can retain the knowledge better and think more coherently. Bigger models also require more VRAM/RAM.</li>
<li>There are several degrees of quantization (8-bit, 5-bit, 4-bit, etc). The lower you go, the more the model degrades, but the lower the hardware requirements. So even on bad hardware, you might be able to run a 4-bit version of your desired model. There&#x27;s even 3-bit and 2-bit quantization but at this point, you&#x27;re beating a dead horse. There&#x27;s also a further quantization subtypes named k_s, k_m, k_l, etc. k_m is better than k_s but requires more resources.</li>
<li>The context size (how long your conversation can become without the model dropping parts of it) also affects VRAM/RAM requirements. Thankfully, this is a configurable setting, allowing you to use a smaller context to reduce VRAM/RAM requirements. (Note: the context size of Llama2-based models is 4k. Mistral is advertised as 8k, but it&#x27;s 4k in practice.)</li>
<li>Sometime in 2023, NVIDIA changed their GPU driver so that if you need more VRAM than your GPU has, instead of the task crashing, it will begin using regular RAM as a fallback. This will ruin the writing speed of the LLM, but the model will still work and give the same quality of output. Thankfully, this behavior <a href="https://nvidia.custhelp.com/app/answers/detail/a_id/5490">can be disabled</a>.</li>
</ul>
<p>Given all of the above, the hardware requirements and performance vary completely depending on the family of model, the type of model, the size of the model, the quantization method, etc.</p>
<doc-anchor-target id="model-size-calculator">
    <h4>
        <doc-anchor-trigger class="header-anchor-trigger" to="#model-size-calculator">#</doc-anchor-trigger>
        <span>Model size calculator</span>
    </h4>
</doc-anchor-target>
<p>You can use <a href="https://huggingface.co/spaces/NyxKrage/LLM-Model-VRAM-Calculator">Nyx&#x27;s Model Size Calculator</a> to determine how much RAM/VRAM you need.</p>
<p>Remember, you want to run the largest, least quantized model that can fit in your memory, i.e. without causing <a href="https://serverfault.com/a/48487">disk swapping</a>.</p>
<doc-anchor-target id="downloading-an-llm">
    <h2>
        <doc-anchor-trigger class="header-anchor-trigger" to="#downloading-an-llm">#</doc-anchor-trigger>
        <span>Downloading an LLM</span>
    </h2>
</doc-anchor-target>
<p>To get started, you will need to download an LLM. The most common place to find and download LLMs is on HuggingFace. There are thousands of models available. A good way to find GGUF models is to check bartowski&#x27;s account page: <a href="https://huggingface.co/bartowski">https://huggingface.co/bartowski</a>. If you don&#x27;t want GGUF, he links the original model page where you might find other formats for that same model.</p>
<p>On a given model&#x27;s page, you will find a whole bunch of files.</p>
<ul>
<li>You might not need all of them! For GGUF, you just need the .gguf model file (usually 4-11GB). If you find multiple large files, it&#x27;s usually all different quantizations of the same model, you only need to pick one.</li>
<li>For .safetensors files (which can be GPTQ or AWQ or HF quantized or unquantized), if you see a number sequence in the filename like model-00001-of-00003.safetensors, then you need all 3 of those .safetensors files + all the other files in the repository (tokenizer, configs, etc.) to get the full model.</li>
<li>As of January 2024, Mixtral MOE 8x7B is widely considered the state of the art for local LLMs. If you have the 32GB of RAM to run it, definitely try it. If you have less than 32GB of RAM, then use Kunoichi-DPO-v2-7B, which despite its size is stellar out of the gate.</li>
</ul>
<doc-anchor-target id="walkthrough-for-downloading-kunoichi-dpo-v2-7b">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#walkthrough-for-downloading-kunoichi-dpo-v2-7b">#</doc-anchor-trigger>
        <span>Walkthrough for downloading Kunoichi-DPO-v2-7B</span>
    </h3>
</doc-anchor-target>
<p>We will use the Kunoichi-DPO-v2-7B model for the rest of this guide. It&#x27;s an excellent model based on Mistral 7B, that only requires 7GB RAM, and punches far above its weight. Note: Kunoichi uses Alpaca prompting.</p>
<ul>
<li>Go to <a href="https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF">https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF</a></li>
<li>Click &#x27;Files and versions&#x27;. You will see a listing of several files. These are all the same model but offered in different quantization options. Click the file &#x27;kunoichi-dpo-v2-7b.Q6_K.gguf&#x27;, which gives us a 6-bit quantization.</li>
<li>Click the &#x27;download&#x27; button. Your download should start.</li>
</ul>
<doc-anchor-target id="how-to-identify-the-type-of-model">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#how-to-identify-the-type-of-model">#</doc-anchor-trigger>
        <span>How to identify the type of model</span>
    </h3>
</doc-anchor-target>
<p>Good model uploaders like TheBloke give descriptive names. But if they don&#x27;t:</p>
<ul>
<li>Filename ends in .gguf: GGUF CPU model (duh)</li>
<li>Filename ends in .safetensors: can be unquantized, or HF quantized, or GPTQ, or AWQ</li>
<li>Filename is pytorch-***.bin: same as above, but this is an older model file format that allows the model to execute arbitrary Python script when the model is loaded, and is considered unsafe. You can still use it if you trust the model creator, or are desperate, but pick .safetensors if you have the option.</li>
<li>config.json exists? Look if it has a quant_method.</li>
<li>q4 means 4-bit quantization, q5 is 5-bit quantization, etc</li>
<li>You see a number like -16k? That&#x27;s an increased context size (i.e. how long your conversation can get before the model forgets the beginning of your chat)! Note that higher context sizes require more VRAM.</li>
</ul>
<doc-anchor-target id="installing-an-llm-server-oobabooga-or-koboldai">
    <h2>
        <doc-anchor-trigger class="header-anchor-trigger" to="#installing-an-llm-server-oobabooga-or-koboldai">#</doc-anchor-trigger>
        <span>Installing an LLM server: Oobabooga or KoboldAI</span>
    </h2>
</doc-anchor-target>
<p>With the LLM now on your PC, we need to download a tool that will act as a middle-man between SillyTavern and the model: it will load the model, and expose its functionality as a local HTTP web API that SillyTavern can talk to, the same way that SillyTavern talks with paid webservices like OpenAI GPT or Claude. The tool you use should be either KoboldAI or Oobabooga (or other compatible tools).</p>
<p>This guide covers both options, you only need one.</p>
<div class="flex mb-6">
    <div class="shrink-0 w-1 rounded-tl rounded-bl bg-yellow-500"></div>
    <div class="flex w-full py-4 border border-l-0 border-gray-300 rounded-tr rounded-br doc-alert bg-white dark:bg-dark-700 dark:border-dark-700" role="alert">
        <div class="flex items-center ml-4 h-7">
            <svg xmlns="http://www.w3.org/2000/svg" class="mb-px text-yellow-500" width="22" height="22" viewBox="0 0 24 24" role="presentation">
                <g fill="currentColor"><g>
                    <path d="M22.48 15.59L14.01 1.45A2.968 2.968 0 0012.16.09c-.78-.19-1.58-.07-2.27.35-.41.25-.76.6-1.01 1.01v.01L.4 15.6c-.83 1.43-.33 3.27 1.1 4.1.45.26.95.4 1.48.4h16.95c.8-.01 1.55-.33 2.11-.9.56-.57.87-1.33.86-2.13a3.04 3.04 0 00-.42-1.48zm-1.87 2.21c-.19.19-.44.3-.69.3H2.99c-.17 0-.34-.05-.49-.13a.992.992 0 01-.37-1.35L10.6 2.48c.08-.14.2-.25.34-.33a.992.992 0 011.37.33l8.46 14.13c.09.15.13.32.13.49 0 .26-.1.51-.29.7z"></path>
                    <path d="M11.45 12.1c.55 0 1-.45 1-1v-4c0-.55-.45-1-1-1s-1 .45-1 1v4c0 .56.45 1 1 1zM11.46 14.1c-.56 0-1 .45-1 1s.45 1 1 1 1-.45 1-1-.45-1-1-1z"></path>
                </g></g>
            </svg>
        </div>
        <div class="pr-5 ml-3 w-full">
<p>If you are hosting SillyTavern on Docker, use <strong><a href="http://host.docker.internal">http://host.docker.internal</a>:<port></strong> instead of <strong><a href="http://127.0.0.1">http://127.0.0.1</a>:<port></strong>. This is because SillyTavern connects to the API endpoint from the server running in the Docker container. Docker&#x27;s network stack is separate from the host&#x27;s, and so the loopback interfaces are not shared.</p>
        </div>
    </div>
</div>
<doc-anchor-target id="downloading-and-using-koboldcpp-no-installation-required-gguf-models">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#downloading-and-using-koboldcpp-no-installation-required-gguf-models">#</doc-anchor-trigger>
        <span>Downloading and using KoboldCpp (No installation required, GGUF models)</span>
    </h3>
</doc-anchor-target>
<ol>
<li>Visit <a href="https://koboldai.org/cpp">https://koboldai.org/cpp</a> where you will see the latest version with various files you can download.
At the time of writing the newest CUDA version they list is cu12 which will work best on modern Nvidia GPU&#x27;s, if you have an older GPU or a different brand you can use the regular koboldcpp.exe. If you have an old CPU its possible that KoboldCpp will crash when you try to load models, in that case try the _oldcpu version to see if it resolves your issue.</li>
<li>KoboldCpp does not need to be installed, once you start KoboldCpp you will immediately be able to select your GGUF model such as the one linked above using the Browse button next to the Model field.</li>
<li>By default KoboldCpp runs at a maximum of 4K context even if you set this higher in SillyTavern, if you wish to run a model at higher context make sure to adjust the context slider on this screen before launching the model. Keep in mind that more context size means higher (video) memory requirements, if you set this to high or load a model that is to big for your system KoboldCpp will automatically begin using your CPU for the layers it can not fit on your GPU, this will be much slower.</li>
<li>Click Launch, if everything goes well a new webpage will open with KoboldAI Lite where you can test if everything works correctly.</li>
<li>Open SillyTavern and click API Connections (2nd button in the top bar)</li>
<li>Set API to Text Completion and the API Type to KoboldCpp.</li>
<li>Set server URL to <a href="http://127.0.0.1:5001/">http://127.0.0.1:5001/</a> or the link that KoboldCpp gave you in case it is not running on the same system (You can activate KoboldCpp&#x27;s Remote Tunnel mode to obtain a link that can be accessed from anywhere).</li>
<li>Click Connect. It should connect successfully and detect kunoichi-dpo-v2-7b.Q6_K.gguf as the model.</li>
<li>Chat with a character to test that it works.</li>
</ol>
<doc-anchor-target id="tips-for-optimizing-koboldcpps-speed">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#tips-for-optimizing-koboldcpps-speed">#</doc-anchor-trigger>
        <span>Tips for Optimizing KoboldCpp&#x27;s speed</span>
    </h3>
</doc-anchor-target>
<ol>
<li>Flash Attention will help reduce the memory requirements, it can be faster or slowing depending on your system and will allow you to fit more layers on your GPU than the default.</li>
<li>KoboldCpp will leave some space for other software when it guesses layers to prevent issues, if you have few programs open and are unable to fit the model entirely in the GPU you may be able to add a few extra layers.</li>
<li>If the model uses up to much memory for the context size you can decrease this by Quantizing the KV. This will reduce the quality of the output but can help you put more layers on the GPU. To do this you go to the Tokens tab in KoboldCpp and then disable Context Shifting and enable Flash Attention. This will unlock the Quantized KV Cache slider, a lower number means less memory / intelligence of the model.</li>
<li>Running KoboldCpp on a slower system where it takes long to process the prompt? Context Shifting works best when you avoid using Lorebooks, randomization or other features that dynamically change the input. Leaving context shifting enabled KoboldCpp will help you avoid long reprocessing times.</li>
</ol>
<doc-anchor-target id="installing-oobabooga">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#installing-oobabooga">#</doc-anchor-trigger>
        <span>Installing Oobabooga</span>
    </h3>
</doc-anchor-target>
<p>Here&#x27;s a more correct/dummy proof installation procedure:</p>
<ol>
<li>git clone <a href="https://github.com/oobabooga/text-generation-webui">https://github.com/oobabooga/text-generation-webui</a> (or download their repo as a .zip in your browser, then extract it)</li>
<li>Run start_windows.bat or whatever your OS is</li>
<li>When asked, select your GPU type. Even if you intend to use GGUF/CPU, if your GPU is in the list, select it now, because it will give you the option to use a speed optimization later called GPU sharding (without having to reinstall from scratch). If you have no gaming-grade dGPU (NVIDIA, AMD), select None.</li>
<li>Wait for the installation to finish</li>
<li>Place kunoichi-dpo-v2-7b.Q6_K.gguf in text-generation-webui/models</li>
<li>Open text-generation-webui/CMD_FLAGS.txt, delete everything inside and write: --api</li>
<li>Restart Oobabooga</li>
<li>Visit <a href="http://127.0.0.1:5000/docs">http://127.0.0.1:5000/docs</a>. Does it load a FastAPI page? If not, you messed up somewhere.</li>
</ol>
<doc-anchor-target id="loading-our-model-in-oobabooga">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#loading-our-model-in-oobabooga">#</doc-anchor-trigger>
        <span>Loading our model in Oobabooga</span>
    </h3>
</doc-anchor-target>
<ol>
<li>Open <a href="http://127.0.0.1:7860/">http://127.0.0.1:7860/</a> in your browser</li>
<li>Click the Model tab</li>
<li>In the dropdown, select our Kunoichi DPO v2  model. It should have automatically selected the llama.cpp loader.</li>
<li>(Optional) We mentioned &#x27;GPU offload&#x27; several times earlier: that&#x27;s the n-gpu-layers setting on this page. If you want to use it, set a value before loading the model. As a basic reference, setting it to 30 uses just under 6GB VRAM for 13B and lower models. (it varies with model architecture and size)</li>
<li>Click Load</li>
</ol>
<doc-anchor-target id="configuring-sillytavern-to-talk-to-oobabooga">
    <h3>
        <doc-anchor-trigger class="header-anchor-trigger" to="#configuring-sillytavern-to-talk-to-oobabooga">#</doc-anchor-trigger>
        <span>Configuring SillyTavern to talk to Oobabooga</span>
    </h3>
</doc-anchor-target>
<ol>
<li>Click API Connections (2nd button in the top bar)</li>
<li>Set API to Text Completion</li>
<li>Set API Type to Default (Oobabooga)</li>
<li>Set server URL to <a href="http://127.0.0.1:5000/">http://127.0.0.1:5000/</a></li>
<li>Click Connect. It should connect successfully and detect kunoichi-dpo-v2-7b.Q6_K.gguf as the model.</li>
<li>Chat with a character to test that it works</li>
</ol>
<doc-anchor-target id="conclusion">
    <h2>
        <doc-anchor-trigger class="header-anchor-trigger" to="#conclusion">#</doc-anchor-trigger>
        <span>Conclusion</span>
    </h2>
</doc-anchor-target>
<p>Congrats, you should now have a working local LLM.</p>

                                
                                <!-- Required only on API pages -->
                                <doc-toolbar-member-filter-no-results></doc-toolbar-member-filter-no-results>
                            </div>
                            <footer class="clear-both">
                                <div class="flex flex-wrap items-center justify-between mt-14">
                                    <a class="my-2.5 inline-flex items-center text-sm whitespace-nowrap text-blue-500 dark:text-blue-400 hover:text-blue-700 hover:underline" href="https://github.com/SillyTavern/SillyTavern-Docs/edit/main/Usage/API_Connections/self-hosted.md" target="_blank" rel="noopener">
                                        <svg class="mr-1.5" xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="currentColor" overflow="visible"><path d="M20 12c-.55 0-1 .45-1 1v7c0 .55-.45 1-1 1H4c-.55 0-1-.45-1-1V6c0-.55.45-1 1-1h7c.55 0 1-.45 1-1s-.45-1-1-1H4C2.35 3 1 4.35 1 6v14c0 1.65 1.35 3 3 3h14c1.65 0 3-1.35 3-3v-7c0-.55-.45-1-1-1z" /><path d="M22.21 1.79c-1.18-1.18-3.24-1.18-4.41 0l-9.5 9.5c-.13.13-.22.29-.26.46l-1 4c-.08.34.01.7.26.95.18.2.44.3.7.3.08 0 .16-.01.24-.03l4-1c.18-.04.34-.13.46-.26l9.5-9.5c1.22-1.22 1.22-3.2.01-4.42zm-1.42 3l-9.3 9.3-2.11.53.53-2.11 9.3-9.3c.42-.42 1.16-.42 1.59 0 .43.43.43 1.15-.01 1.58z" /><path fill="none" d="M0 0h24v24H0z" /></svg>
                                        <span>Edit this page</span>
                                    </a>
                                </div>
                            
                                <nav class="flex mt-14">
                                    <div class="w-1/2">
                                        <a class="px-5 py-4 h-full flex items-center break-normal font-medium text-blue-500 dark:text-blue-400 border border-gray-300 hover:border-gray-400 dark:border-dark-650 dark:hover:border-dark-450 rounded-l-lg transition-colors duration-150 relative hover:z-5" href="../../usage/core-concepts/connection-profiles/">
                                            <svg xmlns="http://www.w3.org/2000/svg" class="mr-3" width="24" height="24" viewBox="0 0 24 24" fill="currentColor" overflow="visible"><path d="M19 11H7.41l5.29-5.29a.996.996 0 10-1.41-1.41l-7 7a1 1 0 000 1.42l7 7a1.024 1.024 0 001.42-.01.996.996 0 000-1.41L7.41 13H19c.55 0 1-.45 1-1s-.45-1-1-1z" /><path fill="none" d="M0 0h24v24H0z" /></svg>
                                            <span>
                                                <span class="block text-xs font-normal text-gray-400 dark:text-dark-400">Previous</span>
                                                <span class="block mt-1">Connection Profiles</span>
                                            </span>
                                        </a>
                                    </div>
                            
                                    <div class="w-1/2">
                                        <a class="px-5 py-4 -mx-px h-full flex items-center justify-end break-normal font-medium text-blue-500 dark:text-blue-400 border border-gray-300 hover:border-gray-400 dark:border-dark-650 dark:hover:border-dark-450 rounded-r-lg transition-colors duration-150 relative hover:z-5" href="../../usage/api-connections/openai/">
                                            <span>
                                                <span class="block text-xs font-normal text-right text-gray-400 dark:text-dark-400">Next</span>
                                                <span class="block mt-1">Chat Completions</span>
                                            </span>
                                            <svg xmlns="http://www.w3.org/2000/svg" class="ml-3" width="24" height="24" viewBox="0 0 24 24" fill="currentColor" overflow="visible"><path d="M19.92 12.38a1 1 0 00-.22-1.09l-7-7a.996.996 0 10-1.41 1.41l5.3 5.3H5c-.55 0-1 .45-1 1s.45 1 1 1h11.59l-5.29 5.29a.996.996 0 000 1.41c.19.2.44.3.7.3s.51-.1.71-.29l7-7c.09-.09.16-.21.21-.33z" /><path fill="none" d="M0 0h24v24H0z" /></svg>
                                        </a>
                                    </div>
                                </nav>
                            </footer>
                        </main>
                
                        <div class="border-t dark:border-dark-650 pt-6 mb-8">
                            <footer class="flex flex-wrap items-center justify-between">
                                <div>
                                    <ul class="flex flex-wrap items-center text-sm">
                                    </ul>
                                </div>
                                <div class="docs-copyright py-2 text-gray-500 dark:text-dark-350 text-sm leading-relaxed"><p>© Copyright 2025. All rights reserved.</p></div>
                            </footer>
                        </div>
                    </div>
                
                    <!-- Rendered if sidebar right is enabled -->
                    <!-- Sidebar right skeleton-->
                    <div v-cloak class="fixed top-0 bottom-0 right-0 translate-x-full bg-white border-gray-200 lg:sticky lg:border-l lg:shrink-0 lg:pt-6 lg:transform-none sm:w-1/2 lg:w-64 lg:z-0 md:w-104 sidebar-right skeleton dark:bg-dark-850 dark:border-dark-650">
                        <div class="pl-5">
                            <div class="w-32 h-3 mb-4 bg-gray-200 dark:bg-dark-600 rounded-full loading"></div>
                            <div class="w-48 h-3 mb-4 bg-gray-200 dark:bg-dark-600 rounded-full loading"></div>
                            <div class="w-40 h-3 mb-4 bg-gray-200 dark:bg-dark-600 rounded-full loading"></div>
                        </div>
                    </div>
                
                    <!-- User should be able to hide sidebar right -->
                    <doc-sidebar-right v-cloak></doc-sidebar-right>
                </div>

            </div>
        </div>
    
        <doc-search-mobile></doc-search-mobile>
        <doc-back-to-top></doc-back-to-top>
    </div>


    <div id="docs-overlay-target"></div>

    <script data-cfasync="false">window.__DOCS__ = { "title": "Self-hosted AI models", level: 2, icon: "file", hasPrism: false, hasMermaid: false, hasMath: false, tocDepth: 23 }</script>
</body>
</html>
